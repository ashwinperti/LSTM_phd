{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMgFQ3rcT1JUnQGHq2iXFQe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashwinperti/LSTM_phd/blob/main/lstm1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "kRDJfAhk9CZE"
      },
      "outputs": [],
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from nltk.corpus import stopwords \n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_csv = 'movie_data.csv'\n",
        "df = pd.read_csv(base_csv, engine='python', error_bad_lines=False)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 313
        },
        "id": "8sHcMnvfCkP3",
        "outputId": "2c6039a0-e28c-4f68-dde8-414e0af9f95a"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "Skipping line 36632: unexpected end of data\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                              review  sentiment\n",
              "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
              "1  OK... so... I really like Kris Kristofferson a...          0\n",
              "2  ***SPOILER*** Do not read this, if you think a...          0\n",
              "3  hi for all the people who have seen this wonde...          1\n",
              "4  I recently bought the DVD, forgetting just how...          0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-62d4a06b-1087-40da-8e17-954c2ea14c71\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>hi for all the people who have seen this wonde...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>I recently bought the DVD, forgetting just how...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-62d4a06b-1087-40da-8e17-954c2ea14c71')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-62d4a06b-1087-40da-8e17-954c2ea14c71 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-62d4a06b-1087-40da-8e17-954c2ea14c71');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = df['review'].values, df['sentiment'].values\n",
        "x_train,x_test,y_train,y_test = train_test_split(X,y,stratify=y)\n",
        "print(f'shape of train data is {x_train.shape}')\n",
        "print(f'shape of test data is {x_test.shape}')\n",
        "dd = pd.Series(y_train).value_counts()\n",
        "sns.barplot(x=np.array(['negative','positive']),y=dd.values)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "TYARe6tQCqee",
        "outputId": "8b56b79b-08d7-4280-e6be-4d76537caf75"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of train data is (27472,)\n",
            "shape of test data is (9158,)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkxDK4tWHRvP",
        "outputId": "8b6ac553-8884-4858-bba3-2e72c817ab81"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_string(s):\n",
        "    # Remove all non-word characters (everything except numbers and letters)\n",
        "    s = re.sub(r\"[^\\w\\s]\", '', s)\n",
        "    # Replace all runs of whitespaces with no space\n",
        "    s = re.sub(r\"\\s+\", '', s)\n",
        "    # replace digits with no space\n",
        "    s = re.sub(r\"\\d\", '', s)\n",
        "\n",
        "    return s\n",
        "\n",
        "def tockenize(x_train,y_train,x_val,y_val):\n",
        "    word_list = []\n",
        "\n",
        "    stop_words = set(stopwords.words('english')) \n",
        "    for sent in x_train:\n",
        "        for word in sent.lower().split():\n",
        "            word = preprocess_string(word)\n",
        "            if word not in stop_words and word != '':\n",
        "                word_list.append(word)\n",
        "  \n",
        "    corpus = Counter(word_list)\n",
        "    # sorting on the basis of most common words\n",
        "    corpus_ = sorted(corpus,key=corpus.get,reverse=True)[:1000]\n",
        "    # creating a dict\n",
        "    onehot_dict = {w:i+1 for i,w in enumerate(corpus_)}\n",
        "    \n",
        "    # tockenize\n",
        "    final_list_train,final_list_test = [],[]\n",
        "    for sent in x_train:\n",
        "            final_list_train.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
        "                                     if preprocess_string(word) in onehot_dict.keys()])\n",
        "    for sent in x_val:\n",
        "            final_list_test.append([onehot_dict[preprocess_string(word)] for word in sent.lower().split() \n",
        "                                    if preprocess_string(word) in onehot_dict.keys()])\n",
        "            \n",
        "    encoded_train = [1 if label =='positive' else 0 for label in y_train]  \n",
        "    encoded_test = [1 if label =='positive' else 0 for label in y_val] \n",
        "    return np.array(final_list_train), np.array(encoded_train),np.array(final_list_test), np.array(encoded_test),onehot_dict\n",
        "x_train,y_train,x_test,y_test,vocab = tockenize(x_train,y_train,x_test,y_test)\n",
        "print(f'Length of vocabulary is {len(vocab)}')\n",
        "\n",
        "rev_len = [len(i) for i in x_train]\n",
        "pd.Series(rev_len).hist()\n",
        "plt.show()\n",
        "pd.Series(rev_len).describe()\n",
        "\n",
        "def padding_(sentences, seq_len):\n",
        "    features = np.zeros((len(sentences), seq_len),dtype=int)\n",
        "    for ii, review in enumerate(sentences):\n",
        "        if len(review) != 0:\n",
        "            features[ii, -len(review):] = np.array(review)[:seq_len]\n",
        "    return features\n",
        "#we have very less number of reviews with length > 500.\n",
        "#So we will consideronly those below it.\n",
        "x_train_pad = padding_(x_train,500)\n",
        "x_test_pad = padding_(x_test,500)\n",
        "\n",
        "# create Tensor datasets\n",
        "train_data = TensorDataset(torch.from_numpy(x_train_pad), torch.from_numpy(y_train))\n",
        "valid_data = TensorDataset(torch.from_numpy(x_test_pad), torch.from_numpy(y_test))\n",
        "\n",
        "# dataloaders\n",
        "batch_size = 50\n",
        "\n",
        "# make sure to SHUFFLE your data\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "\n",
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "sample_x, sample_y = dataiter.next()\n",
        "\n",
        "print('Sample input size: ', sample_x.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', sample_x)\n",
        "print('Sample input: \\n', sample_y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "Xu3-ZPfSCq4P",
        "outputId": "5c0b3ca5-1dd1-4be8-db8e-c989fedaede0"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of vocabulary is 1000\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample input size:  torch.Size([50, 500])\n",
            "Sample input: \n",
            " tensor([[  0,   0,   0,  ..., 336,  67,   7],\n",
            "        [  0,   0,   0,  ..., 798, 826, 288],\n",
            "        [  0,   0,   0,  ..., 831,   3, 846],\n",
            "        ...,\n",
            "        [  0,   0,   0,  ..., 568, 432,   1],\n",
            "        [  0,   0,   0,  ...,  60,   5, 525],\n",
            "        [  0,   0,   0,  ...,  47, 606,   3]])\n",
            "Sample input: \n",
            " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentRNN(nn.Module):\n",
        "    def __init__(self,no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5):\n",
        "        super(SentimentRNN,self).__init__()\n",
        " \n",
        "        self.output_dim = output_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        " \n",
        "        self.no_layers = no_layers\n",
        "        self.vocab_size = vocab_size\n",
        "    \n",
        "        # embedding and LSTM layers\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        #lstm\n",
        "        self.lstm = nn.LSTM(input_size=embedding_dim,hidden_size=self.hidden_dim,\n",
        "                           num_layers=no_layers, batch_first=True)\n",
        "        \n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "    \n",
        "        # linear and sigmoid layer\n",
        "        self.fc = nn.Linear(self.hidden_dim, output_dim)\n",
        "        self.sig = nn.Sigmoid()\n",
        "        \n",
        "    def forward(self,x,hidden):\n",
        "        batch_size = x.size(0)\n",
        "        # embeddings and lstm_out\n",
        "        embeds = self.embedding(x)  # shape: B x S x Feature   since batch = True\n",
        "        #print(embeds.shape)  #[50, 500, 1000]\n",
        "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
        "        \n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim) \n",
        "        \n",
        "        # dropout and fully connected layer\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        \n",
        "        # sigmoid function\n",
        "        sig_out = self.sig(out)\n",
        "        \n",
        "        # reshape to be batch_size first\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "\n",
        "        sig_out = sig_out[:, -1] # get last batch of labels\n",
        "        \n",
        "        # return last sigmoid output and hidden state\n",
        "        return sig_out, hidden\n",
        "        \n",
        "        \n",
        "    def init_hidden(self, batch_size):\n",
        "        ''' Initializes hidden state '''\n",
        "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
        "        # initialized to zero, for hidden state and cell state of LSTM\n",
        "        h0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        c0 = torch.zeros((self.no_layers,batch_size,self.hidden_dim)).to(device)\n",
        "        hidden = (h0,c0)\n",
        "        return hidden\n",
        "is_cuda = torch.cuda.is_available()\n",
        "\n",
        "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
        "if is_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, CPU used\")\n",
        "\n",
        "no_layers = 2\n",
        "vocab_size = len(vocab) + 1 #extra 1 for padding\n",
        "embedding_dim = 64\n",
        "output_dim = 1\n",
        "hidden_dim = 256\n",
        "\n",
        "\n",
        "model = SentimentRNN(no_layers,vocab_size,hidden_dim,embedding_dim,drop_prob=0.5)\n",
        "\n",
        "#moving to gpu\n",
        "model.to(device)\n",
        "\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNHmlojPCkq7",
        "outputId": "5712c8c0-d52b-4224-d154-6951ff189094"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n",
            "SentimentRNN(\n",
            "  (embedding): Embedding(1001, 64)\n",
            "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            "  (sig): Sigmoid()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss and optimization functions\n",
        "lr=0.001\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# function to predict accuracy\n",
        "def acc(pred,label):\n",
        "    pred = torch.round(pred.squeeze())\n",
        "    return torch.sum(pred == label.squeeze()).item()\n",
        "clip = 5\n",
        "epochs = 5 \n",
        "valid_loss_min = np.Inf\n",
        "# train for some number of epochs\n",
        "epoch_tr_loss,epoch_vl_loss = [],[]\n",
        "epoch_tr_acc,epoch_vl_acc = [],[]\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    train_losses = []\n",
        "    train_acc = 0.0\n",
        "    model.train()\n",
        "    # initialize hidden state \n",
        "    h = model.init_hidden(batch_size)\n",
        "    for inputs, labels in train_loader:\n",
        "        \n",
        "        inputs, labels = inputs.to(device), labels.to(device)   \n",
        "        # Creating new variables for the hidden state, otherwise\n",
        "        # we'd backprop through the entire training history\n",
        "        h = tuple([each.data for each in h])\n",
        "        \n",
        "        model.zero_grad()\n",
        "        output,h = model(inputs,h)\n",
        "        \n",
        "        # calculate the loss and perform backprop\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        train_losses.append(loss.item())\n",
        "        # calculating accuracy\n",
        "        accuracy = acc(output,labels)\n",
        "        train_acc += accuracy\n",
        "        #`clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        " \n",
        "    \n",
        "        \n",
        "    val_h = model.init_hidden(batch_size)\n",
        "    val_losses = []\n",
        "    val_acc = 0.0\n",
        "    model.eval()\n",
        "    for inputs, labels in valid_loader:\n",
        "            val_h = tuple([each.data for each in val_h])\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            output, val_h = model(inputs, val_h)\n",
        "            val_loss = criterion(output.squeeze(), labels.float())\n",
        "\n",
        "            val_losses.append(val_loss.item())\n",
        "            \n",
        "            accuracy = acc(output,labels)\n",
        "            val_acc += accuracy\n",
        "            \n",
        "    epoch_train_loss = np.mean(train_losses)\n",
        "    epoch_val_loss = np.mean(val_losses)\n",
        "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
        "    epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
        "    epoch_tr_loss.append(epoch_train_loss)\n",
        "    epoch_vl_loss.append(epoch_val_loss)\n",
        "    epoch_tr_acc.append(epoch_train_acc)\n",
        "    epoch_vl_acc.append(epoch_val_acc)\n",
        "    print(f'Epoch {epoch+1}') \n",
        "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
        "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
        "    if epoch_val_loss <= valid_loss_min:\n",
        "        torch.save(model.state_dict(), 'working/state_dict.pt')\n",
        "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
        "        valid_loss_min = epoch_val_loss\n",
        "    print(25*'==')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZVPLaPoDdvU",
        "outputId": "7d228153-ec01-44dc-fa5f-62c70cc865b7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "train_loss : 0.34262346759438517 val_loss : 0.339274450302124\n",
            "train_accuracy : 85.488 val_accuracy : 85.14399999999999\n",
            "Validation loss decreased (inf --> 0.339274).  Saving model ...\n",
            "==================================================\n",
            "Epoch 2\n",
            "train_loss : 0.30891206276416777 val_loss : 0.3290041715502739\n",
            "train_accuracy : 86.856 val_accuracy : 86.032\n",
            "Validation loss decreased (0.339274 --> 0.329004).  Saving model ...\n",
            "==================================================\n",
            "Epoch 3\n",
            "train_loss : 0.2797532272736232 val_loss : 0.3245178155899048\n",
            "train_accuracy : 88.53866666666667 val_accuracy : 86.088\n",
            "Validation loss decreased (0.329004 --> 0.324518).  Saving model ...\n",
            "==================================================\n",
            "Epoch 4\n",
            "train_loss : 0.2383811497737964 val_loss : 0.3501507088840008\n",
            "train_accuracy : 90.35466666666667 val_accuracy : 85.96000000000001\n",
            "==================================================\n",
            "Epoch 5\n",
            "train_loss : 0.18359137377639612 val_loss : 0.3981481117606163\n",
            "train_accuracy : 92.896 val_accuracy : 84.464\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fig = plt.figure(figsize = (20, 6))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epoch_tr_acc, label='Train Acc')\n",
        "plt.plot(epoch_vl_acc, label='Validation Acc')\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "    \n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epoch_tr_loss, label='Train loss')\n",
        "plt.plot(epoch_vl_loss, label='Validation loss')\n",
        "plt.title(\"Loss\")\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "\n",
        "plt.show()\n",
        "plt.savefig(\"model1.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "unfu6sCTDlAe",
        "outputId": "aacc37ae-4ee1-464d-c3cd-85087c2c301f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x432 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_text(text):\n",
        "        word_seq = np.array([vocab[preprocess_string(word)] for word in text.split() \n",
        "                         if preprocess_string(word) in vocab.keys()])\n",
        "        word_seq = np.expand_dims(word_seq,axis=0)\n",
        "        pad =  torch.from_numpy(padding_(word_seq,500))\n",
        "        inputs = pad.to(device)\n",
        "        batch_size = 1\n",
        "        h = model.init_hidden(batch_size)\n",
        "        h = tuple([each.data for each in h])\n",
        "        output, h = model(inputs, h)\n",
        "        return(output.item())\n",
        "\n",
        "index = 30\n",
        "print(df['review'][index])\n",
        "print('='*70)\n",
        "print(f'Actual sentiment is  : {df[\"sentiment\"][index]}')\n",
        "print('='*70)\n",
        "pro = predict_text(df['review'][index])\n",
        "status = \"positive\" if pro > 0.5 else \"negative\"\n",
        "pro = (1 - pro) if status == \"negative\" else pro\n",
        "print(f'Predicted sentiment is {status} with a probability of {pro}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8re24gj8Dd5M",
        "outputId": "14849b8f-5356-4368-c58e-21d9e1176813"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Taut and organically gripping, Edward Dmytryk's Crossfire is a distinctive suspense thriller, an unlikely \"message\" movie using the look and devices of the noir cycle.<br /><br />Bivouacked in Washington, DC, a company of soldiers cope with their restlessness by hanging out in bars. Three of them end up at a stranger's apartment where Robert Ryan, drunk and belligerent, beats their host (Sam Levene) to death because he happens to be Jewish. Police detective Robert Young investigates with the help of Robert Mitchum, who's assigned to Ryan's outfit. Suspicion falls on the second of the three (George Cooper), who has vanished. Ryan slays the third buddy (Steve Brodie) to insure his silence before Young closes in.<br /><br />Abetted by a superior script by John Paxton, Dmytryk draws precise performances from his three starring Bobs. Ryan, naturally, does his prototypical Angry White Male (and to the hilt), while Mitchum underplays with his characteristic alert nonchalance (his role, however, is not central); Young may never have been better. Gloria Grahame gives her first fully-fledged rendition of the smart-mouthed, vulnerable tramp, and, as a sad sack who's leeched into her life, Paul Kelly haunts us in a small, peripheral role that he makes memorable.<br /><br />The politically engaged Dmytryk perhaps inevitably succumbs to sermonizing, but it's pretty much confined to Young's reminiscence of how his Irish grandfather died at the hands of bigots a century earlier (thus, incidentally, stretching chronology to the limit). At least there's no attempt to render an explanation, however glib, of why Ryan hates Jews (and hillbillies and...).<br /><br />Curiously, Crossfire survives even the major change wrought upon it -- the novel it's based on (Richard Brooks' The Brick Foxhole) dealt with a gay-bashing murder. But homosexuality in 1947 was still Beyond The Pale. News of the Holocaust had, however, begun to emerge from the ashes of Europe, so Hollywood felt emboldened to register its protest against anti-Semitism (the studios always quaked at the prospect of offending any potential ticket buyer).<br /><br />But while the change from homophobia to anti-Semitism works in general, the specifics don't fit so smoothly. The victim's chatting up a lonesome, drunk young soldier then inviting him back home looks odd, even though (or especially since) there's a girlfriend in tow. It raises the question whether this scenario was retained inadvertently or left in as a discreet tip-off to the original engine generating Ryan's murderous rage.\n",
            "======================================================================\n",
            "Actual sentiment is  : positive\n",
            "======================================================================\n",
            "Predicted sentiment is positive with a probability of 0.9103707671165466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index = 32\n",
        "print(df['review'][index])\n",
        "print('='*70)\n",
        "print(f'Actual sentiment is  : {df[\"sentiment\"][index]}')\n",
        "print('='*70)\n",
        "pro = predict_text(df['review'][index])\n",
        "status = \"positive\" if pro > 0.5 else \"negative\"\n",
        "pro = (1 - pro) if status == \"negative\" else pro\n",
        "print(f'predicted sentiment is {status} with a probability of {pro}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JP_mbXa8DeDt",
        "outputId": "cc46a864-89a5-4675-f88f-40a7483fbfef"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My first exposure to the Templarios & not a good one. I was excited to find this title among the offerings from Anchor Bay Video, which has brought us other cult classics such as \"Spider Baby\". The print quality is excellent, but this alone can't hide the fact that the film is deadly dull. There's a thrilling opening sequence in which the villagers exact a terrible revenge on the Templars (& set the whole thing in motion), but everything else in the movie is slow, ponderous &, ultimately, unfulfilling. Adding insult to injury: the movie was dubbed, not subtitled, as promised on the video jacket.\n",
            "======================================================================\n",
            "Actual sentiment is  : negative\n",
            "======================================================================\n",
            "predicted sentiment is negative with a probability of 0.9705355875194073\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyprind"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzSZzngxPZnm",
        "outputId": "d59a0907-8e73-4647-c80d-c970b6d9ada8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyprind\n",
        "import pandas as pd\n",
        "from string import punctuation\n",
        "import re\n",
        "import numpy as np\n",
        "df = pd.read_csv('IMDB.csv', encoding='utf-8')\n",
        "\n",
        "from collections import Counter\n",
        "counts = Counter()\n",
        "pbar = pyprind.ProgBar(len(df['review']), \\\n",
        "            title='Counting words occurrences')\n",
        "\n",
        "for i,review in enumerate(df['review']):\n",
        "    text = ''.join([c if c not in punctuation else ' '+c+' ' \\\n",
        "                            for c in review]).lower()\n",
        "    df.loc[i,'review'] = text\n",
        "    pbar.update()\n",
        "    counts.update(text.split())\n",
        "\n",
        "## Create a mapping\n",
        "## Map each unique word to an integer\n",
        "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
        "print(word_counts[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naj168uQHmBr",
        "outputId": "9f0d2535-613e-40cd-b195-4e1f94cef0d9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Counting words occurrences\n",
            "0% [##############################] 100% | ETA: 00:00:00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', '.', ',', 'and', 'a']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Total time elapsed: 00:00:22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
        "\n",
        "mapped_reviews = []\n",
        "pbar = pyprind.ProgBar(len(df['review']), \\\n",
        "                 title='Map reviews to ints')\n",
        "for review in df['review']:\n",
        "        mapped_reviews.append([word_to_int[word] \\\n",
        "                    for word in review.split()])\n",
        "        pbar.update()\n",
        "\n",
        "## Define same-length sequences\n",
        "## if sequence length < 200: left-pad with zeros\n",
        "## if sequence length > 200: use the last 200 elements\n",
        "\n",
        "sequence_length = 200 ## (Known as T in our RNN formulas)\n",
        "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
        "\n",
        "for i, row in enumerate(mapped_reviews):\n",
        "         review_arr = np.array(row)\n",
        "         sequences[i, -len(row):] = review_arr[-sequence_length:]\n",
        "\n",
        "X_train = sequences[:25000,:]\n",
        "y_train = df.loc[:25000, 'sentiment'].values\n",
        "X_test = sequences[25000:,:]\n",
        "y_test = df.loc[25000:, 'sentiment'].values\n",
        "\n",
        "np.random.seed(123) # for reproducibility\n",
        "## Define a function to generate mini-batches:\n",
        "def create_batch_generator(x, y=None, batch_size=64):\n",
        "    n_batches = len(x)//batch_size\n",
        "    x = x[:n_batches*batch_size]\n",
        "    if y is not None:\n",
        "        y = y[:n_batches*batch_size]\n",
        "    for ii in range(0, len(x), batch_size):\n",
        "        if y is not None:\n",
        "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
        "        else:\n",
        "            yield x[ii:ii+batch_size]\n",
        "import tensorflow as tf\n",
        "class SentimentRNN(object):\n",
        "    def __init__(self, n_words, seq_len=200, lstm_size=256, num_layers=1, batch_size=64,\n",
        "                learning_rate=0.0001, embed_size=200):\n",
        "        self.n_words = n_words\n",
        "        self.seq_len = seq_len\n",
        "        self.lstm_size = lstm_size ## number of hidden units\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.embed_size = embed_size\n",
        "        \n",
        "        self.g = tf.Graph()\n",
        "        with self.g.as_default():\n",
        "            #tf.set_random_seed(123)\n",
        "            self.build()\n",
        "            self.saver = tf.compat.v1.train.Saver()\n",
        "            self.init_op = tf.compat.v1.global_variables_initializer()\n",
        "    \n",
        "    def build(self):\n",
        "        ## Define the placeholders\n",
        "        tf_x = tf.compat.v1.placeholder(tf.int32, shape=(self.batch_size, self.seq_len), name='tf_x')\n",
        "        tf_y = tf.compat.v1.placeholder(tf.float32, shape=(self.batch_size), name='tf_y')\n",
        "        tf_keepprob = tf.compat.v1.placeholder(tf.float32, name='tf_keepprob')\n",
        "        \n",
        "        ## Create the embedding layer\n",
        "        embedding = tf.Variable(tf.random.uniform((self.n_words, self.embed_size),\n",
        "                                minval=-1, maxval=1), name='embedding')\n",
        "        embed_x = tf.nn.embedding_lookup(embedding, tf_x, name='embeded_x')\n",
        "        ## Define LSTM cell and stack them together\n",
        "        cells = tf.compat.v1.nn.rnn_cell.MultiRNNCell(\n",
        "                        [tf.compat.v1.nn.rnn_cell.DropoutWrapper( \n",
        "                                        tf.compat.v1.nn.rnn_cell.BasicLSTMCell(self.lstm_size),\n",
        "                                        output_keep_prob=tf_keepprob)\n",
        "                                        for i in range(self.num_layers)])\n",
        "        ## Define the initial state:\n",
        "        self.initial_state = cells.zero_state(self.batch_size, tf.float32)\n",
        "        print(' << initial state >> ', self.initial_state)\n",
        "        lstm_outputs, self.final_state = tf.compat.v1.nn.dynamic_rnn(cells, embed_x,\n",
        "                                                initial_state=self.initial_state)\n",
        "        ## Note: lstm_outputs shape:\n",
        "        ## [batch_size, max_time, cells.output_size]\n",
        "        print('\\n << lstm_output >> ', lstm_outputs)\n",
        "        print('\\n << final state >> ', self.final_state)\n",
        "        logits = tf.compat.v1.layers.dense(inputs=lstm_outputs[:, -1],\n",
        "                            units=1, activation=None, name='logits')\n",
        "        logits = tf.squeeze(logits, name='logits_squeezed')\n",
        "        print ('\\n << logits >> ', logits)\n",
        "        y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
        "        predictions = {\n",
        "                'probabilities': y_proba,\n",
        "                'labels' : tf.cast(tf.round(y_proba), tf.int32,\n",
        "                name='labels')}\n",
        "        print('\\n << predictions >> ', predictions)\n",
        "        \n",
        "        ## Define the cost function\n",
        "        cost = tf.reduce_mean(\n",
        "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "                labels=tf_y, logits=logits), name='cost')\n",
        "        ## Define the optimizer\n",
        "        optimizer = tf.compat.v1.train.AdamOptimizer(self.learning_rate) \n",
        "        train_op = optimizer.minimize(cost, name='train_op')\n",
        "        \n",
        "    def train(self, X_train, y_train, num_epochs):\n",
        "        with tf.compat.v1.Session(graph=self.g) as sess:\n",
        "            sess.run(self.init_op)\n",
        "            iteration = 1\n",
        "            for epoch in range(num_epochs):\n",
        "                state = sess.run(self.initial_state)\n",
        "                \n",
        "                for batch_x, batch_y in create_batch_generator(\n",
        "                        X_train, y_train, self.batch_size):\n",
        "                    feed = {'tf_x:0': batch_x, 'tf_y:0': batch_y,\n",
        "                        'tf_keepprob:0': 0.5, self.initial_state : state}\n",
        "                    loss, _, state = sess.run(\n",
        "                        ['cost:0', 'train_op', self.final_state],\n",
        "                        feed_dict=feed)\n",
        "                    if iteration % 20 == 0:\n",
        "                        print(\"Epoch: %d/%d Iteration: %d \"\n",
        "                                \"| Train loss: %.5f\" % (\n",
        "                        epoch + 1, num_epochs, iteration, loss))\n",
        "                    iteration += 1\n",
        "                if (epoch+1)%10 == 0:\n",
        "                    self.saver.save(sess,\"model/sentiment-%d.ckpt\" % epoch)\n",
        "        \n",
        "    def predict(self, X_data, return_proba=False):\n",
        "        preds = []\n",
        "        with tf.compat.v1.Session(graph = self.g) as sess:\n",
        "            self.saver.restore(\n",
        "                sess, tf.train.latest_checkpoint('./model/'))\n",
        "            test_state = sess.run(self.initial_state)\n",
        "            for ii, batch_x in enumerate(create_batch_generator(\n",
        "                    X_data, None, batch_size=self.batch_size), 1):\n",
        "                feed = {'tf_x:0' : batch_x,\n",
        "                            'tf_keepprob:0' : 1.0,\n",
        "                        self.initial_state : test_state}\n",
        "                if return_proba:\n",
        "                    pred, test_state = sess.run(\n",
        "                            ['probabilities:0', self.final_state], feed_dict=feed)\n",
        "                else:\n",
        "                    pred, test_state = sess.run(\n",
        "                            ['labels:0', self.final_state],feed_dict=feed)\n",
        "                preds.append(pred)\n",
        "        return np.concatenate(preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jqiv46PrPpzI",
        "outputId": "1cae1655-de4c-4185-c0ec-7c3d485230ae"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map reviews to ints\n",
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_words = max(list(word_to_int.values())) + 1\n",
        "\n",
        "rnn = SentimentRNN(n_words=n_words,\n",
        " seq_len=sequence_length,\n",
        " embed_size=256, lstm_size=128,\n",
        "    num_layers=1,\n",
        "    batch_size=100,\n",
        "    learning_rate=0.001)\n",
        "\n",
        "rnn.train(X_train, y_train, num_epochs=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "id": "BABZzdbaPqBe",
        "outputId": "1a60847d-9f5b-4448-e373-19f41f373b3d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
            " << initial state >>  (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
            "WARNING:tensorflow:From <ipython-input-23-dd11686b3a34>:76: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:760: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:71: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:756: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  shape=[input_depth + h_depth, 4 * self._num_units])\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:760: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  initializer=tf.compat.v1.zeros_initializer(dtype=self.dtype))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " << lstm_output >>  Tensor(\"rnn/transpose_1:0\", shape=(100, 200, 128), dtype=float32)\n",
            "\n",
            " << final state >>  (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Identity_4:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Identity_5:0' shape=(100, 128) dtype=float32>),)\n",
            "\n",
            " << logits >>  Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
            "\n",
            " << predictions >>  {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:82: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-2cece6c46fa7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     learning_rate=0.001)\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-23-dd11686b3a34>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X_train, y_train, num_epochs)\u001b[0m\n\u001b[1;32m    111\u001b[0m                     loss, _, state = sess.run(\n\u001b[1;32m    112\u001b[0m                         \u001b[0;34m[\u001b[0m\u001b[0;34m'cost:0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'train_op'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinal_state\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                         feed_dict=feed)\n\u001b[0m\u001b[1;32m    114\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m20\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                         print(\"Epoch: %d/%d Iteration: %d \"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 968\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1158\u001b[0m             \u001b[0mfeed_handles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubfeed_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1160\u001b[0;31m             \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m           if (not is_tensor_handle_feed and\n",
            "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'positive'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Test: \n",
        "preds = rnn.predict(X_test)\n",
        "y_true = y_test[:len(preds)]\n",
        "print('Test Acc.: %.3f' % (\n",
        "      np.sum(preds == y_true) / len(y_true)))"
      ],
      "metadata": {
        "id": "-Ptd4yBzQNUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Get probabilities:\n",
        "proba = rnn.predict(X_test, return_proba=True)"
      ],
      "metadata": {
        "id": "fa8SUD35QNY4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "class SentimentRNN(object):\n",
        "  def __init__(self, n_words, seq_len=200, lstm_size=256, num_layers=1, batch_size=64,learning_rate=0.0001, embed_size=200):\n",
        "    self.n_words = n_words\n",
        "    self.seq_len = seq_len\n",
        "    self.lstm_size = lstm_size ## number of hidden units\n",
        "    self.num_layers = num_layers\n",
        "    self.batch_size = batch_size\n",
        "    self.learning_rate = learning_rate\n",
        "    self.embed_size = embed_size\n",
        "    self.g = tf.Graph()\n",
        "    with self.g.as_default():\n",
        "        tf.set_random_seed(123)\n",
        "        self.build()\n",
        "        self.saver = tf.train.Saver()\n",
        "        self.init_op = tf.global_variables_initializer()"
      ],
      "metadata": {
        "id": "Py1sWLuuQvJJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyprind"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUYU4JlIwPYv",
        "outputId": "9cacbc39-d731-431a-c5a3-5422f41230d6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyprind\n",
            "  Downloading PyPrind-2.11.3-py2.py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: pyprind\n",
            "Successfully installed pyprind-2.11.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyprind\n",
        "import pandas as pd\n",
        "from string import punctuation\n",
        "import re\n",
        "import numpy as np\n",
        "df = pd.read_csv('movie_data.csv', encoding='utf-8',engine='python',error_bad_lines=False)\n",
        "\n",
        "from collections import Counter\n",
        "counts = Counter()\n",
        "pbar = pyprind.ProgBar(len(df['review']), \\\n",
        "            title='Counting words occurrences')\n",
        "\n",
        "for i,review in enumerate(df['review']):\n",
        "    text = ''.join([c if c not in punctuation else ' '+c+' ' \\\n",
        "                            for c in review]).lower()\n",
        "    df.loc[i,'review'] = text\n",
        "    pbar.update()\n",
        "    counts.update(text.split())\n",
        "\n",
        "## Create a mapping\n",
        "## Map each unique word to an integer\n",
        "word_counts = sorted(counts, key=counts.get, reverse=True)\n",
        "print(word_counts[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzP8wAWGdXWo",
        "outputId": "51fda2e5-fb01-45ee-8e0f-a949671b9e2f"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2882: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
            "\n",
            "\n",
            "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "Skipping line 49330: unexpected end of data\n",
            "Counting words occurrences\n",
            "0% [##############################] 100% | ETA: 00:00:00"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', '.', ',', 'and', 'a']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Total time elapsed: 00:03:10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_to_int = {word: ii for ii, word in enumerate(word_counts, 1)}\n",
        "\n",
        "mapped_reviews = []\n",
        "pbar = pyprind.ProgBar(len(df['review']), \\\n",
        "                 title='Map reviews to ints')\n",
        "for review in df['review']:\n",
        "        mapped_reviews.append([word_to_int[word] \\\n",
        "                    for word in review.split()])\n",
        "        pbar.update()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmANPhFh3u2q",
        "outputId": "47b54b3e-71df-40dd-9966-34da871ce6b6"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Map reviews to ints\n",
            "0% [##############################] 100% | ETA: 00:00:00\n",
            "Total time elapsed: 00:00:03\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(mapped_reviews)\n",
        "print(mapped_reviews[0])\n",
        "\n",
        "## Define same-length sequences\n",
        "## if sequence length < 200: left-pad with zeros\n",
        "## if sequence length > 200: use the last 200 elements\n",
        "\n",
        "sequence_length = 200 ## (Known as T in our RNN formulas)\n",
        "sequences = np.zeros((len(mapped_reviews), sequence_length), dtype=int)\n",
        "\n",
        "for i, row in enumerate(mapped_reviews):\n",
        "         review_arr = np.array(row)\n",
        "         sequences[i, -len(row):] = review_arr[-sequence_length:]\n",
        "#print(sequences.itemsize) #4\n",
        "#sequences.shape # (50000, 200)\n",
        "#type(sequences.shape) # Tuple\n",
        "print(sequences[:,1:])\n",
        "np.add.reduce(sequences[:,1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTWmlV1K29b6",
        "outputId": "c2d5b648-8c96-4282-846e-e6ddfbe7afe0"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15, 5585, 3, 1, 2169, 3948, 26773, 30, 4816, 1571, 29, 1131, 7, 1, 316, 19, 718, 1600, 6, 6625, 752, 3, 14661, 3, 8675, 2, 33, 1, 14662, 330, 3, 3618, 6, 2089, 3, 67, 22, 1951, 15, 1, 8676, 6, 54, 340, 4, 54, 604, 4744, 15507, 2, 1723, 19, 121, 170, 323, 3, 1, 565, 950, 25577, 30, 1466, 19491, 29, 3, 47, 10, 5, 1120, 1099, 1361, 18, 58, 3035, 15, 5976, 25, 44066, 15, 717, 2, 1550, 2, 5237, 3299, 4, 1662, 7, 21358, 3, 1106, 7, 3896, 1, 436, 26, 37, 1963, 1798, 2329, 30, 3654, 3279, 29, 26, 1, 1286, 6, 507, 5, 285, 2, 1, 5495, 10585, 4, 92, 34, 2575, 107, 3, 27, 26, 1, 1458, 6, 1, 5153, 1361, 1323, 8988, 30, 623, 12589, 29, 18, 22, 15, 2740, 6, 1, 3502, 15, 1, 1355, 8, 21, 3, 44, 1887, 1, 1805, 4, 5, 5027, 6, 659, 4, 306, 7, 1003, 1, 604, 2, 12, 13, 9, 11, 12, 13, 9, 11, 20, 604, 15, 14661, 20, 10, 5, 62, 255, 24, 3, 26, 1, 308, 76, 6, 5, 604, 6, 5, 3529, 170, 174, 260, 18, 22, 2377, 45, 5, 3044, 2169, 654, 412, 22, 5, 3169, 2, 1, 973, 4, 993, 245, 347, 79, 2416, 7, 1003, 1, 604, 25, 65, 86, 1723, 170, 2, 206, 3, 5, 39589, 1361, 4, 6257, 62914, 15, 5976, 22, 494, 7, 24562, 99, 1, 4449, 830, 22, 2377, 2, 1, 927, 294, 1, 3502, 6, 950, 4, 1, 254, 499, 6, 3948, 15, 4216, 3, 27, 51, 10, 5, 586, 6, 1, 1432, 15, 1, 12394, 2, 71, 2075, 10, 1588, 2, 12, 13, 9, 11, 12, 13, 9, 11, 439, 30, 3562, 29, 85, 34, 1305]\n",
            "[[1963 1798 2329 ...   85   34 1305]\n",
            " [7372  256  173 ...    1 1974   41]\n",
            " [ 587    7 3067 ...  175   29    2]\n",
            " ...\n",
            " [   0    0    0 ...   17   24    2]\n",
            " [3003   74   10 ...   66 3719    2]\n",
            " [  90  273  937 ... 1458  852    2]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 47364824,  49620701,  48166239,  47981716,  51358747,  48765857,\n",
              "        48744551,  48920028,  51297483,  50794059,  50059348,  52812494,\n",
              "        51917045,  54168965,  53518434,  53146471,  52627910,  52274234,\n",
              "        52899786,  54507531,  54286833,  54962740,  54717488,  54396502,\n",
              "        54755150,  56457658,  54285463,  56882378,  58412575,  57459787,\n",
              "        59937541,  57455679,  58191982,  60385403,  59860741,  59942899,\n",
              "        62521768,  59855906,  60644229,  61025531,  61688458,  61641634,\n",
              "        62479124,  61985891,  61584483,  64071877,  65319780,  62478217,\n",
              "        66123618,  65945090,  65334826,  63197483,  65590829,  67340444,\n",
              "        70382960,  68085061,  69943932,  69519770,  69954537,  68223057,\n",
              "        68918653,  69951102,  68622999,  68589535,  69785776,  70187915,\n",
              "        72495213,  73581861,  72833443,  71693759,  74915245,  70903744,\n",
              "        71614369,  74826032,  74821747,  72835572,  71270335,  73991233,\n",
              "        73144853,  73282995,  72108328,  73468637,  73016988,  75048177,\n",
              "        73122798,  74074614,  75434480,  73951876,  76957783,  75414881,\n",
              "        75711857,  76378153,  73916404,  74317915,  72855077,  75055835,\n",
              "        74074745,  73664559,  76888260,  75526899,  75774921,  77865019,\n",
              "        77474549,  74589342,  76312969,  76481560,  75944980,  76474518,\n",
              "        76433093,  77147799,  76646612,  75151946,  75146504,  75039223,\n",
              "        76751174,  77134242,  77108240,  78536760,  77560643,  76675401,\n",
              "        76828236,  75080000,  77627133,  75998789,  76888093,  75247620,\n",
              "        77652286,  78038415,  75768483,  74451542,  78854790,  78005062,\n",
              "        78030536,  77896432,  78432539,  77278258,  76613627,  77872825,\n",
              "        78857738,  76590472,  77418707,  74306641,  76720173,  76839164,\n",
              "        74723311,  75285865,  75832873,  77533198,  78428865,  74663240,\n",
              "        75826072,  77049281,  74512128,  72574735,  76169594,  74536301,\n",
              "        75624234,  75669070,  75092925,  78742032,  78296837,  74242681,\n",
              "        72346515,  72149918,  73120506,  73373761,  73495710,  71479626,\n",
              "        70834988,  72325096,  71315382,  73590098,  70643755,  69545123,\n",
              "        72530267,  68817413,  69905474,  68696792,  66825034,  70955916,\n",
              "        70083125,  69491930,  67098848,  72247641,  67637165,  70284647,\n",
              "        71948369,  67490482,  71051162,  71304930,  66904362,  67277498,\n",
              "        67741104,  67759435,  76194446,  63372306,  79750368, 127616058,\n",
              "        30647660])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = sequences[:25000,:]\n",
        "y_train = df.loc[:25000, 'sentiment'].values\n",
        "X_test = sequences[25000:,:]\n",
        "y_test = df.loc[25000:, 'sentiment'].values\n",
        "\n",
        "np.random.seed(123) # for reproducibility\n",
        "## Define a function to generate mini-batches:\n",
        "def create_batch_generator(x, y=None, batch_size=64):\n",
        "    n_batches = len(x)//batch_size\n",
        "    x = x[:n_batches*batch_size]\n",
        "    if y is not None:\n",
        "        y = y[:n_batches*batch_size]\n",
        "    for ii in range(0, len(x), batch_size):\n",
        "        if y is not None:\n",
        "            yield x[ii:ii+batch_size], y[ii:ii+batch_size]\n",
        "        else:\n",
        "            yield x[ii:ii+batch_size]"
      ],
      "metadata": {
        "id": "LImzuXf9hm_R"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "class SentimentRNN(object):\n",
        "    def __init__(self, n_words, seq_len=200, lstm_size=256, num_layers=1, batch_size=64,\n",
        "                learning_rate=0.0001, embed_size=200):\n",
        "        self.n_words = n_words\n",
        "        self.seq_len = seq_len\n",
        "        self.lstm_size = lstm_size ## number of hidden units\n",
        "        self.num_layers = num_layers\n",
        "        self.batch_size = batch_size\n",
        "        self.learning_rate = learning_rate\n",
        "        self.embed_size = embed_size\n",
        "        \n",
        "        self.g = tf.Graph()\n",
        "        with self.g.as_default():\n",
        "            #tf.set_random_seed(123)\n",
        "            self.build()\n",
        "            self.saver = tf.compat.v1.train.Saver()\n",
        "            self.init_op = tf.compat.v1.global_variables_initializer()\n",
        "    \n",
        "    def build(self):\n",
        "        ## Define the placeholders\n",
        "        tf_x = tf.compat.v1.placeholder(tf.int32, shape=(self.batch_size, self.seq_len), name='tf_x')\n",
        "        tf_y = tf.compat.v1.placeholder(tf.float32, shape=(self.batch_size), name='tf_y')\n",
        "        tf_keepprob = tf.compat.v1.placeholder(tf.float32, name='tf_keepprob')\n",
        "        \n",
        "        ## Create the embedding layer\n",
        "        embedding = tf.Variable(tf.random.uniform((self.n_words, self.embed_size),\n",
        "                                minval=-1, maxval=1), name='embedding')\n",
        "        embed_x = tf.nn.embedding_lookup(embedding, tf_x, name='embeded_x')\n",
        "        ## Define LSTM cell and stack them together\n",
        "        cells = tf.compat.v1.nn.rnn_cell.MultiRNNCell(\n",
        "                        [tf.compat.v1.nn.rnn_cell.DropoutWrapper( \n",
        "                                        tf.compat.v1.nn.rnn_cell.BasicLSTMCell(self.lstm_size),\n",
        "                                        output_keep_prob=tf_keepprob)\n",
        "                                        for i in range(self.num_layers)])\n",
        "        ## Define the initial state:\n",
        "        self.initial_state = cells.zero_state(self.batch_size, tf.float32)\n",
        "        print(' << initial state >> ', self.initial_state)\n",
        "        lstm_outputs, self.final_state = tf.compat.v1.nn.dynamic_rnn(cells, embed_x,\n",
        "                                                initial_state=self.initial_state)\n",
        "        ## Note: lstm_outputs shape:\n",
        "        ## [batch_size, max_time, cells.output_size]\n",
        "        print('\\n << lstm_output >> ', lstm_outputs)\n",
        "        print('\\n << final state >> ', self.final_state)\n",
        "        logits = tf.compat.v1.layers.dense(inputs=lstm_outputs[:, -1],\n",
        "                            units=1, activation=None, name='logits')\n",
        "        logits = tf.squeeze(logits, name='logits_squeezed')\n",
        "        print ('\\n << logits >> ', logits)\n",
        "        y_proba = tf.nn.sigmoid(logits, name='probabilities')\n",
        "        predictions = {\n",
        "                'probabilities': y_proba,\n",
        "                'labels' : tf.cast(tf.round(y_proba), tf.int32,\n",
        "                name='labels')}\n",
        "        print('\\n << predictions >> ', predictions)\n",
        "        \n",
        "        ## Define the cost function\n",
        "        cost = tf.reduce_mean(\n",
        "                tf.nn.sigmoid_cross_entropy_with_logits(\n",
        "                labels=tf_y, logits=logits), name='cost')\n",
        "        ## Define the optimizer\n",
        "        optimizer = tf.compat.v1.train.AdamOptimizer(self.learning_rate) \n",
        "        train_op = optimizer.minimize(cost, name='train_op')\n",
        "        \n",
        "    def train(self, X_train, y_train, num_epochs):\n",
        "        with tf.compat.v1.Session(graph=self.g) as sess:\n",
        "            sess.run(self.init_op)\n",
        "            iteration = 1\n",
        "            for epoch in range(num_epochs):\n",
        "                state = sess.run(self.initial_state)\n",
        "                \n",
        "                for batch_x, batch_y in create_batch_generator(\n",
        "                        X_train, y_train, self.batch_size):\n",
        "                    feed = {'tf_x:0': batch_x, 'tf_y:0': batch_y,\n",
        "                        'tf_keepprob:0': 0.5, self.initial_state : state}\n",
        "                    loss, _, state = sess.run(\n",
        "                        ['cost:0', 'train_op', self.final_state],\n",
        "                        feed_dict=feed)\n",
        "                    if iteration % 20 == 0:\n",
        "                        print(\"Epoch: %d/%d Iteration: %d \"\n",
        "                                \"| Train loss: %.5f\" % (\n",
        "                        epoch + 1, num_epochs, iteration, loss))\n",
        "                    iteration += 1\n",
        "                if (epoch+1)%10 == 0:\n",
        "                    self.saver.save(sess,\"model/sentiment-%d.ckpt\" % epoch)\n",
        "        \n",
        "    def predict(self, X_data, return_proba=False):\n",
        "        preds = []\n",
        "        with tf.compat.v1.Session(graph = self.g) as sess:\n",
        "            self.saver.restore(\n",
        "                sess, tf.train.latest_checkpoint('./model/'))\n",
        "            test_state = sess.run(self.initial_state)\n",
        "            for ii, batch_x in enumerate(create_batch_generator(\n",
        "                    X_data, None, batch_size=self.batch_size), 1):\n",
        "                feed = {'tf_x:0' : batch_x,\n",
        "                            'tf_keepprob:0' : 1.0,\n",
        "                        self.initial_state : test_state}\n",
        "                if return_proba:\n",
        "                    pred, test_state = sess.run(\n",
        "                            ['probabilities:0', self.final_state], feed_dict=feed)\n",
        "                else:\n",
        "                    pred, test_state = sess.run(\n",
        "                            ['labels:0', self.final_state],feed_dict=feed)\n",
        "                preds.append(pred)\n",
        "        return np.concatenate(preds)"
      ],
      "metadata": {
        "id": "OYkSgIrki3tg"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_words = max(list(word_to_int.values())) + 1\n",
        "\n",
        "rnn = SentimentRNN(n_words=n_words,\n",
        " seq_len=sequence_length,\n",
        " embed_size=256, lstm_size=128,\n",
        "    num_layers=1,\n",
        "    batch_size=100,\n",
        "    learning_rate=0.001)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rm74dg5sdXlZ",
        "outputId": "8cc59490-b882-4191-ef0f-77a0db0be36a"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:756: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  shape=[input_depth + h_depth, 4 * self._num_units])\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:760: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  initializer=tf.compat.v1.zeros_initializer(dtype=self.dtype))\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:46: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:261: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " << initial state >>  (LSTMStateTuple(c=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'MultiRNNCellZeroState/DropoutWrapperZeroState/BasicLSTMCellZeroState/zeros_1:0' shape=(100, 128) dtype=float32>),)\n",
            "\n",
            " << lstm_output >>  Tensor(\"rnn/transpose_1:0\", shape=(100, 200, 128), dtype=float32)\n",
            "\n",
            " << final state >>  (LSTMStateTuple(c=<tf.Tensor 'rnn/while/Identity_4:0' shape=(100, 128) dtype=float32>, h=<tf.Tensor 'rnn/while/Identity_5:0' shape=(100, 128) dtype=float32>),)\n",
            "\n",
            " << logits >>  Tensor(\"logits_squeezed:0\", shape=(100,), dtype=float32)\n",
            "\n",
            " << predictions >>  {'probabilities': <tf.Tensor 'probabilities:0' shape=(100,) dtype=float32>, 'labels': <tf.Tensor 'labels:0' shape=(100,) dtype=int32>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wy3tHRKLgbBG",
        "outputId": "3b969ea9-44ea-4225-85a1-727ad6fbe852"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, ..., 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rnn.train(X_train, y_train, num_epochs=40)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmJvBxfMgXKW",
        "outputId": "12357e1e-f104-444f-ee39-406b4d0908a1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/40 Iteration: 20 | Train loss: 0.69310\n",
            "Epoch: 1/40 Iteration: 40 | Train loss: 0.63718\n",
            "Epoch: 1/40 Iteration: 60 | Train loss: 0.65936\n",
            "Epoch: 1/40 Iteration: 80 | Train loss: 0.56854\n",
            "Epoch: 1/40 Iteration: 100 | Train loss: 0.51929\n",
            "Epoch: 1/40 Iteration: 120 | Train loss: 0.54169\n",
            "Epoch: 1/40 Iteration: 140 | Train loss: 0.58231\n",
            "Epoch: 1/40 Iteration: 160 | Train loss: 0.54350\n",
            "Epoch: 1/40 Iteration: 180 | Train loss: 0.62344\n",
            "Epoch: 1/40 Iteration: 200 | Train loss: 0.52899\n",
            "Epoch: 1/40 Iteration: 220 | Train loss: 0.42484\n",
            "Epoch: 1/40 Iteration: 240 | Train loss: 0.44939\n",
            "Epoch: 2/40 Iteration: 260 | Train loss: 0.46309\n",
            "Epoch: 2/40 Iteration: 280 | Train loss: 0.35847\n",
            "Epoch: 2/40 Iteration: 300 | Train loss: 0.35217\n",
            "Epoch: 2/40 Iteration: 320 | Train loss: 0.39671\n",
            "Epoch: 2/40 Iteration: 340 | Train loss: 0.31790\n",
            "Epoch: 2/40 Iteration: 360 | Train loss: 0.28340\n",
            "Epoch: 2/40 Iteration: 380 | Train loss: 0.26748\n",
            "Epoch: 2/40 Iteration: 400 | Train loss: 0.37216\n",
            "Epoch: 2/40 Iteration: 420 | Train loss: 0.40802\n",
            "Epoch: 2/40 Iteration: 440 | Train loss: 0.37271\n",
            "Epoch: 2/40 Iteration: 460 | Train loss: 0.37257\n",
            "Epoch: 2/40 Iteration: 480 | Train loss: 0.27864\n",
            "Epoch: 2/40 Iteration: 500 | Train loss: 0.19360\n",
            "Epoch: 3/40 Iteration: 520 | Train loss: 0.34098\n",
            "Epoch: 3/40 Iteration: 540 | Train loss: 0.24016\n",
            "Epoch: 3/40 Iteration: 560 | Train loss: 0.39472\n",
            "Epoch: 3/40 Iteration: 580 | Train loss: 0.26547\n",
            "Epoch: 3/40 Iteration: 600 | Train loss: 0.29174\n",
            "Epoch: 3/40 Iteration: 620 | Train loss: 0.32444\n",
            "Epoch: 3/40 Iteration: 640 | Train loss: 0.39469\n",
            "Epoch: 3/40 Iteration: 660 | Train loss: 0.17736\n",
            "Epoch: 3/40 Iteration: 680 | Train loss: 0.29486\n",
            "Epoch: 3/40 Iteration: 700 | Train loss: 0.18239\n",
            "Epoch: 3/40 Iteration: 720 | Train loss: 0.20581\n",
            "Epoch: 3/40 Iteration: 740 | Train loss: 0.26709\n",
            "Epoch: 4/40 Iteration: 760 | Train loss: 0.23306\n",
            "Epoch: 4/40 Iteration: 780 | Train loss: 0.13560\n",
            "Epoch: 4/40 Iteration: 800 | Train loss: 0.21697\n",
            "Epoch: 4/40 Iteration: 820 | Train loss: 0.25521\n",
            "Epoch: 4/40 Iteration: 840 | Train loss: 0.16101\n",
            "Epoch: 4/40 Iteration: 860 | Train loss: 0.10466\n",
            "Epoch: 4/40 Iteration: 880 | Train loss: 0.21287\n",
            "Epoch: 4/40 Iteration: 900 | Train loss: 0.21693\n",
            "Epoch: 4/40 Iteration: 920 | Train loss: 0.22718\n",
            "Epoch: 4/40 Iteration: 940 | Train loss: 0.25692\n",
            "Epoch: 4/40 Iteration: 960 | Train loss: 0.27130\n",
            "Epoch: 4/40 Iteration: 980 | Train loss: 0.17423\n",
            "Epoch: 4/40 Iteration: 1000 | Train loss: 0.14009\n",
            "Epoch: 5/40 Iteration: 1020 | Train loss: 0.17960\n",
            "Epoch: 5/40 Iteration: 1040 | Train loss: 0.08874\n",
            "Epoch: 5/40 Iteration: 1060 | Train loss: 0.19564\n",
            "Epoch: 5/40 Iteration: 1080 | Train loss: 0.13156\n",
            "Epoch: 5/40 Iteration: 1100 | Train loss: 0.14976\n",
            "Epoch: 5/40 Iteration: 1120 | Train loss: 0.13977\n",
            "Epoch: 5/40 Iteration: 1140 | Train loss: 0.14709\n",
            "Epoch: 5/40 Iteration: 1160 | Train loss: 0.07534\n",
            "Epoch: 5/40 Iteration: 1180 | Train loss: 0.16137\n",
            "Epoch: 5/40 Iteration: 1200 | Train loss: 0.11100\n",
            "Epoch: 5/40 Iteration: 1220 | Train loss: 0.17002\n",
            "Epoch: 5/40 Iteration: 1240 | Train loss: 0.08444\n",
            "Epoch: 6/40 Iteration: 1260 | Train loss: 0.07385\n",
            "Epoch: 6/40 Iteration: 1280 | Train loss: 0.10166\n",
            "Epoch: 6/40 Iteration: 1300 | Train loss: 0.11300\n",
            "Epoch: 6/40 Iteration: 1320 | Train loss: 0.13643\n",
            "Epoch: 6/40 Iteration: 1340 | Train loss: 0.07269\n",
            "Epoch: 6/40 Iteration: 1360 | Train loss: 0.27259\n",
            "Epoch: 6/40 Iteration: 1380 | Train loss: 0.21087\n",
            "Epoch: 6/40 Iteration: 1400 | Train loss: 0.16201\n",
            "Epoch: 6/40 Iteration: 1420 | Train loss: 0.11724\n",
            "Epoch: 6/40 Iteration: 1440 | Train loss: 0.15370\n",
            "Epoch: 6/40 Iteration: 1460 | Train loss: 0.32324\n",
            "Epoch: 6/40 Iteration: 1480 | Train loss: 0.18280\n",
            "Epoch: 6/40 Iteration: 1500 | Train loss: 0.10414\n",
            "Epoch: 7/40 Iteration: 1520 | Train loss: 0.13194\n",
            "Epoch: 7/40 Iteration: 1540 | Train loss: 0.04394\n",
            "Epoch: 7/40 Iteration: 1560 | Train loss: 0.17742\n",
            "Epoch: 7/40 Iteration: 1580 | Train loss: 0.11829\n",
            "Epoch: 7/40 Iteration: 1600 | Train loss: 0.05184\n",
            "Epoch: 7/40 Iteration: 1620 | Train loss: 0.16244\n",
            "Epoch: 7/40 Iteration: 1640 | Train loss: 0.07573\n",
            "Epoch: 7/40 Iteration: 1660 | Train loss: 0.02206\n",
            "Epoch: 7/40 Iteration: 1680 | Train loss: 0.10384\n",
            "Epoch: 7/40 Iteration: 1700 | Train loss: 0.02815\n",
            "Epoch: 7/40 Iteration: 1720 | Train loss: 0.14451\n",
            "Epoch: 7/40 Iteration: 1740 | Train loss: 0.11363\n",
            "Epoch: 8/40 Iteration: 1760 | Train loss: 0.06984\n",
            "Epoch: 8/40 Iteration: 1780 | Train loss: 0.07285\n",
            "Epoch: 8/40 Iteration: 1800 | Train loss: 0.06349\n",
            "Epoch: 8/40 Iteration: 1820 | Train loss: 0.08384\n",
            "Epoch: 8/40 Iteration: 1840 | Train loss: 0.02519\n",
            "Epoch: 8/40 Iteration: 1860 | Train loss: 0.03465\n",
            "Epoch: 8/40 Iteration: 1880 | Train loss: 0.09692\n",
            "Epoch: 8/40 Iteration: 1900 | Train loss: 0.14812\n",
            "Epoch: 8/40 Iteration: 1920 | Train loss: 0.08172\n",
            "Epoch: 8/40 Iteration: 1940 | Train loss: 0.09480\n",
            "Epoch: 8/40 Iteration: 1960 | Train loss: 0.08174\n",
            "Epoch: 8/40 Iteration: 1980 | Train loss: 0.09103\n",
            "Epoch: 8/40 Iteration: 2000 | Train loss: 0.11966\n",
            "Epoch: 9/40 Iteration: 2020 | Train loss: 0.10207\n",
            "Epoch: 9/40 Iteration: 2040 | Train loss: 0.04018\n",
            "Epoch: 9/40 Iteration: 2060 | Train loss: 0.01054\n",
            "Epoch: 9/40 Iteration: 2080 | Train loss: 0.03629\n",
            "Epoch: 9/40 Iteration: 2100 | Train loss: 0.02426\n",
            "Epoch: 9/40 Iteration: 2120 | Train loss: 0.07323\n",
            "Epoch: 9/40 Iteration: 2140 | Train loss: 0.00463\n",
            "Epoch: 9/40 Iteration: 2160 | Train loss: 0.01699\n",
            "Epoch: 9/40 Iteration: 2180 | Train loss: 0.01525\n",
            "Epoch: 9/40 Iteration: 2200 | Train loss: 0.05577\n",
            "Epoch: 9/40 Iteration: 2220 | Train loss: 0.04852\n",
            "Epoch: 9/40 Iteration: 2240 | Train loss: 0.04119\n",
            "Epoch: 10/40 Iteration: 2260 | Train loss: 0.01064\n",
            "Epoch: 10/40 Iteration: 2280 | Train loss: 0.05981\n",
            "Epoch: 10/40 Iteration: 2300 | Train loss: 0.05446\n",
            "Epoch: 10/40 Iteration: 2320 | Train loss: 0.18134\n",
            "Epoch: 10/40 Iteration: 2340 | Train loss: 0.04833\n",
            "Epoch: 10/40 Iteration: 2360 | Train loss: 0.03877\n",
            "Epoch: 10/40 Iteration: 2380 | Train loss: 0.04274\n",
            "Epoch: 10/40 Iteration: 2400 | Train loss: 0.05759\n",
            "Epoch: 10/40 Iteration: 2420 | Train loss: 0.07168\n",
            "Epoch: 10/40 Iteration: 2440 | Train loss: 0.01367\n",
            "Epoch: 10/40 Iteration: 2460 | Train loss: 0.01824\n",
            "Epoch: 10/40 Iteration: 2480 | Train loss: 0.01676\n",
            "Epoch: 10/40 Iteration: 2500 | Train loss: 0.02874\n",
            "Epoch: 11/40 Iteration: 2520 | Train loss: 0.03340\n",
            "Epoch: 11/40 Iteration: 2540 | Train loss: 0.00458\n",
            "Epoch: 11/40 Iteration: 2560 | Train loss: 0.00671\n",
            "Epoch: 11/40 Iteration: 2580 | Train loss: 0.03114\n",
            "Epoch: 11/40 Iteration: 2600 | Train loss: 0.01236\n",
            "Epoch: 11/40 Iteration: 2620 | Train loss: 0.06431\n",
            "Epoch: 11/40 Iteration: 2640 | Train loss: 0.01300\n",
            "Epoch: 11/40 Iteration: 2660 | Train loss: 0.00779\n",
            "Epoch: 11/40 Iteration: 2680 | Train loss: 0.01176\n",
            "Epoch: 11/40 Iteration: 2700 | Train loss: 0.04359\n",
            "Epoch: 11/40 Iteration: 2720 | Train loss: 0.07553\n",
            "Epoch: 11/40 Iteration: 2740 | Train loss: 0.04840\n",
            "Epoch: 12/40 Iteration: 2760 | Train loss: 0.01038\n",
            "Epoch: 12/40 Iteration: 2780 | Train loss: 0.01641\n",
            "Epoch: 12/40 Iteration: 2800 | Train loss: 0.01915\n",
            "Epoch: 12/40 Iteration: 2820 | Train loss: 0.00448\n",
            "Epoch: 12/40 Iteration: 2840 | Train loss: 0.00567\n",
            "Epoch: 12/40 Iteration: 2860 | Train loss: 0.00586\n",
            "Epoch: 12/40 Iteration: 2880 | Train loss: 0.03416\n",
            "Epoch: 12/40 Iteration: 2900 | Train loss: 0.00532\n",
            "Epoch: 12/40 Iteration: 2920 | Train loss: 0.09501\n",
            "Epoch: 12/40 Iteration: 2940 | Train loss: 0.10743\n",
            "Epoch: 12/40 Iteration: 2960 | Train loss: 0.07290\n",
            "Epoch: 12/40 Iteration: 2980 | Train loss: 0.01021\n",
            "Epoch: 12/40 Iteration: 3000 | Train loss: 0.04707\n",
            "Epoch: 13/40 Iteration: 3020 | Train loss: 0.02281\n",
            "Epoch: 13/40 Iteration: 3040 | Train loss: 0.00932\n",
            "Epoch: 13/40 Iteration: 3060 | Train loss: 0.00205\n",
            "Epoch: 13/40 Iteration: 3080 | Train loss: 0.00828\n",
            "Epoch: 13/40 Iteration: 3100 | Train loss: 0.00996\n",
            "Epoch: 13/40 Iteration: 3120 | Train loss: 0.10587\n",
            "Epoch: 13/40 Iteration: 3140 | Train loss: 0.04648\n",
            "Epoch: 13/40 Iteration: 3160 | Train loss: 0.01259\n",
            "Epoch: 13/40 Iteration: 3180 | Train loss: 0.01581\n",
            "Epoch: 13/40 Iteration: 3200 | Train loss: 0.00913\n",
            "Epoch: 13/40 Iteration: 3220 | Train loss: 0.08228\n",
            "Epoch: 13/40 Iteration: 3240 | Train loss: 0.00478\n",
            "Epoch: 14/40 Iteration: 3260 | Train loss: 0.00352\n",
            "Epoch: 14/40 Iteration: 3280 | Train loss: 0.00290\n",
            "Epoch: 14/40 Iteration: 3300 | Train loss: 0.00276\n",
            "Epoch: 14/40 Iteration: 3320 | Train loss: 0.01568\n",
            "Epoch: 14/40 Iteration: 3340 | Train loss: 0.06017\n",
            "Epoch: 14/40 Iteration: 3360 | Train loss: 0.02795\n",
            "Epoch: 14/40 Iteration: 3380 | Train loss: 0.03040\n",
            "Epoch: 14/40 Iteration: 3400 | Train loss: 0.00680\n",
            "Epoch: 14/40 Iteration: 3420 | Train loss: 0.06338\n",
            "Epoch: 14/40 Iteration: 3440 | Train loss: 0.00751\n",
            "Epoch: 14/40 Iteration: 3460 | Train loss: 0.01443\n",
            "Epoch: 14/40 Iteration: 3480 | Train loss: 0.04362\n",
            "Epoch: 14/40 Iteration: 3500 | Train loss: 0.03784\n",
            "Epoch: 15/40 Iteration: 3520 | Train loss: 0.00473\n",
            "Epoch: 15/40 Iteration: 3540 | Train loss: 0.00128\n",
            "Epoch: 15/40 Iteration: 3560 | Train loss: 0.00703\n",
            "Epoch: 15/40 Iteration: 3580 | Train loss: 0.02397\n",
            "Epoch: 15/40 Iteration: 3600 | Train loss: 0.01693\n",
            "Epoch: 15/40 Iteration: 3620 | Train loss: 0.01757\n",
            "Epoch: 15/40 Iteration: 3640 | Train loss: 0.00523\n",
            "Epoch: 15/40 Iteration: 3660 | Train loss: 0.00235\n",
            "Epoch: 15/40 Iteration: 3680 | Train loss: 0.00854\n",
            "Epoch: 15/40 Iteration: 3700 | Train loss: 0.01484\n",
            "Epoch: 15/40 Iteration: 3720 | Train loss: 0.04456\n",
            "Epoch: 15/40 Iteration: 3740 | Train loss: 0.00227\n",
            "Epoch: 16/40 Iteration: 3760 | Train loss: 0.01972\n",
            "Epoch: 16/40 Iteration: 3780 | Train loss: 0.00149\n",
            "Epoch: 16/40 Iteration: 3800 | Train loss: 0.02648\n",
            "Epoch: 16/40 Iteration: 3820 | Train loss: 0.00274\n",
            "Epoch: 16/40 Iteration: 3840 | Train loss: 0.00539\n",
            "Epoch: 16/40 Iteration: 3860 | Train loss: 0.00179\n",
            "Epoch: 16/40 Iteration: 3880 | Train loss: 0.01390\n",
            "Epoch: 16/40 Iteration: 3900 | Train loss: 0.01036\n",
            "Epoch: 16/40 Iteration: 3920 | Train loss: 0.06305\n",
            "Epoch: 16/40 Iteration: 3940 | Train loss: 0.06875\n",
            "Epoch: 16/40 Iteration: 3960 | Train loss: 0.00073\n",
            "Epoch: 16/40 Iteration: 3980 | Train loss: 0.00307\n",
            "Epoch: 16/40 Iteration: 4000 | Train loss: 0.00575\n",
            "Epoch: 17/40 Iteration: 4020 | Train loss: 0.00596\n",
            "Epoch: 17/40 Iteration: 4040 | Train loss: 0.01650\n",
            "Epoch: 17/40 Iteration: 4060 | Train loss: 0.01079\n",
            "Epoch: 17/40 Iteration: 4080 | Train loss: 0.00056\n",
            "Epoch: 17/40 Iteration: 4100 | Train loss: 0.00167\n",
            "Epoch: 17/40 Iteration: 4120 | Train loss: 0.02855\n",
            "Epoch: 17/40 Iteration: 4140 | Train loss: 0.01053\n",
            "Epoch: 17/40 Iteration: 4160 | Train loss: 0.00441\n",
            "Epoch: 17/40 Iteration: 4180 | Train loss: 0.00102\n",
            "Epoch: 17/40 Iteration: 4200 | Train loss: 0.00043\n",
            "Epoch: 17/40 Iteration: 4220 | Train loss: 0.00111\n",
            "Epoch: 17/40 Iteration: 4240 | Train loss: 0.00652\n",
            "Epoch: 18/40 Iteration: 4260 | Train loss: 0.00094\n",
            "Epoch: 18/40 Iteration: 4280 | Train loss: 0.00150\n",
            "Epoch: 18/40 Iteration: 4300 | Train loss: 0.00034\n",
            "Epoch: 18/40 Iteration: 4320 | Train loss: 0.00514\n",
            "Epoch: 18/40 Iteration: 4340 | Train loss: 0.03713\n",
            "Epoch: 18/40 Iteration: 4360 | Train loss: 0.00153\n",
            "Epoch: 18/40 Iteration: 4380 | Train loss: 0.24848\n",
            "Epoch: 18/40 Iteration: 4400 | Train loss: 0.01385\n",
            "Epoch: 18/40 Iteration: 4420 | Train loss: 0.04394\n",
            "Epoch: 18/40 Iteration: 4440 | Train loss: 0.00653\n",
            "Epoch: 18/40 Iteration: 4460 | Train loss: 0.01093\n",
            "Epoch: 18/40 Iteration: 4480 | Train loss: 0.00492\n",
            "Epoch: 18/40 Iteration: 4500 | Train loss: 0.06528\n",
            "Epoch: 19/40 Iteration: 4520 | Train loss: 0.03376\n",
            "Epoch: 19/40 Iteration: 4540 | Train loss: 0.00185\n",
            "Epoch: 19/40 Iteration: 4560 | Train loss: 0.11241\n",
            "Epoch: 19/40 Iteration: 4580 | Train loss: 0.11838\n",
            "Epoch: 19/40 Iteration: 4600 | Train loss: 0.10164\n",
            "Epoch: 19/40 Iteration: 4620 | Train loss: 0.03248\n",
            "Epoch: 19/40 Iteration: 4640 | Train loss: 0.00722\n",
            "Epoch: 19/40 Iteration: 4660 | Train loss: 0.00297\n",
            "Epoch: 19/40 Iteration: 4680 | Train loss: 0.00784\n",
            "Epoch: 19/40 Iteration: 4700 | Train loss: 0.01529\n",
            "Epoch: 19/40 Iteration: 4720 | Train loss: 0.01875\n",
            "Epoch: 19/40 Iteration: 4740 | Train loss: 0.00644\n",
            "Epoch: 20/40 Iteration: 4760 | Train loss: 0.00215\n",
            "Epoch: 20/40 Iteration: 4780 | Train loss: 0.00251\n",
            "Epoch: 20/40 Iteration: 4800 | Train loss: 0.00225\n",
            "Epoch: 20/40 Iteration: 4820 | Train loss: 0.10248\n",
            "Epoch: 20/40 Iteration: 4840 | Train loss: 0.00635\n",
            "Epoch: 20/40 Iteration: 4860 | Train loss: 0.00195\n",
            "Epoch: 20/40 Iteration: 4880 | Train loss: 0.04621\n",
            "Epoch: 20/40 Iteration: 4900 | Train loss: 0.03192\n",
            "Epoch: 20/40 Iteration: 4920 | Train loss: 0.02564\n",
            "Epoch: 20/40 Iteration: 4940 | Train loss: 0.01915\n",
            "Epoch: 20/40 Iteration: 4960 | Train loss: 0.03001\n",
            "Epoch: 20/40 Iteration: 4980 | Train loss: 0.00726\n",
            "Epoch: 20/40 Iteration: 5000 | Train loss: 0.13660\n",
            "Epoch: 21/40 Iteration: 5020 | Train loss: 0.00797\n",
            "Epoch: 21/40 Iteration: 5040 | Train loss: 0.00285\n",
            "Epoch: 21/40 Iteration: 5060 | Train loss: 0.00623\n",
            "Epoch: 21/40 Iteration: 5080 | Train loss: 0.07423\n",
            "Epoch: 21/40 Iteration: 5100 | Train loss: 0.01210\n",
            "Epoch: 21/40 Iteration: 5120 | Train loss: 0.00388\n",
            "Epoch: 21/40 Iteration: 5140 | Train loss: 0.00064\n",
            "Epoch: 21/40 Iteration: 5160 | Train loss: 0.00142\n",
            "Epoch: 21/40 Iteration: 5180 | Train loss: 0.00227\n",
            "Epoch: 21/40 Iteration: 5200 | Train loss: 0.00155\n",
            "Epoch: 21/40 Iteration: 5220 | Train loss: 0.00706\n",
            "Epoch: 21/40 Iteration: 5240 | Train loss: 0.00228\n",
            "Epoch: 22/40 Iteration: 5260 | Train loss: 0.01068\n",
            "Epoch: 22/40 Iteration: 5280 | Train loss: 0.00033\n",
            "Epoch: 22/40 Iteration: 5300 | Train loss: 0.00143\n",
            "Epoch: 22/40 Iteration: 5320 | Train loss: 0.00233\n",
            "Epoch: 22/40 Iteration: 5340 | Train loss: 0.00081\n",
            "Epoch: 22/40 Iteration: 5360 | Train loss: 0.00064\n",
            "Epoch: 22/40 Iteration: 5380 | Train loss: 0.00183\n",
            "Epoch: 22/40 Iteration: 5400 | Train loss: 0.00148\n",
            "Epoch: 22/40 Iteration: 5420 | Train loss: 0.02557\n",
            "Epoch: 22/40 Iteration: 5440 | Train loss: 0.00099\n",
            "Epoch: 22/40 Iteration: 5460 | Train loss: 0.00046\n",
            "Epoch: 22/40 Iteration: 5480 | Train loss: 0.00230\n",
            "Epoch: 22/40 Iteration: 5500 | Train loss: 0.00068\n",
            "Epoch: 23/40 Iteration: 5520 | Train loss: 0.00261\n",
            "Epoch: 23/40 Iteration: 5540 | Train loss: 0.00036\n",
            "Epoch: 23/40 Iteration: 5560 | Train loss: 0.00087\n",
            "Epoch: 23/40 Iteration: 5580 | Train loss: 0.01081\n",
            "Epoch: 23/40 Iteration: 5600 | Train loss: 0.00029\n",
            "Epoch: 23/40 Iteration: 5620 | Train loss: 0.00032\n",
            "Epoch: 23/40 Iteration: 5640 | Train loss: 0.00021\n",
            "Epoch: 23/40 Iteration: 5660 | Train loss: 0.00031\n",
            "Epoch: 23/40 Iteration: 5680 | Train loss: 0.00034\n",
            "Epoch: 23/40 Iteration: 5700 | Train loss: 0.00032\n",
            "Epoch: 23/40 Iteration: 5720 | Train loss: 0.00018\n",
            "Epoch: 23/40 Iteration: 5740 | Train loss: 0.00049\n",
            "Epoch: 24/40 Iteration: 5760 | Train loss: 0.00042\n",
            "Epoch: 24/40 Iteration: 5780 | Train loss: 0.00035\n",
            "Epoch: 24/40 Iteration: 5800 | Train loss: 0.00022\n",
            "Epoch: 24/40 Iteration: 5820 | Train loss: 0.00027\n",
            "Epoch: 24/40 Iteration: 5840 | Train loss: 0.00206\n",
            "Epoch: 24/40 Iteration: 5860 | Train loss: 0.00020\n",
            "Epoch: 24/40 Iteration: 5880 | Train loss: 0.00029\n",
            "Epoch: 24/40 Iteration: 5900 | Train loss: 0.00188\n",
            "Epoch: 24/40 Iteration: 5920 | Train loss: 0.03017\n",
            "Epoch: 24/40 Iteration: 5940 | Train loss: 0.00020\n",
            "Epoch: 24/40 Iteration: 5960 | Train loss: 0.00010\n",
            "Epoch: 24/40 Iteration: 5980 | Train loss: 0.00012\n",
            "Epoch: 24/40 Iteration: 6000 | Train loss: 0.00020\n",
            "Epoch: 25/40 Iteration: 6020 | Train loss: 0.00276\n",
            "Epoch: 25/40 Iteration: 6040 | Train loss: 0.00029\n",
            "Epoch: 25/40 Iteration: 6060 | Train loss: 0.00040\n",
            "Epoch: 25/40 Iteration: 6080 | Train loss: 0.00027\n",
            "Epoch: 25/40 Iteration: 6100 | Train loss: 0.00040\n",
            "Epoch: 25/40 Iteration: 6120 | Train loss: 0.00008\n",
            "Epoch: 25/40 Iteration: 6140 | Train loss: 0.00005\n",
            "Epoch: 25/40 Iteration: 6160 | Train loss: 0.00020\n",
            "Epoch: 25/40 Iteration: 6180 | Train loss: 0.00009\n",
            "Epoch: 25/40 Iteration: 6200 | Train loss: 0.00011\n",
            "Epoch: 25/40 Iteration: 6220 | Train loss: 0.00014\n",
            "Epoch: 25/40 Iteration: 6240 | Train loss: 0.00015\n",
            "Epoch: 26/40 Iteration: 6260 | Train loss: 0.00019\n",
            "Epoch: 26/40 Iteration: 6280 | Train loss: 0.00007\n",
            "Epoch: 26/40 Iteration: 6300 | Train loss: 0.00004\n",
            "Epoch: 26/40 Iteration: 6320 | Train loss: 0.00011\n",
            "Epoch: 26/40 Iteration: 6340 | Train loss: 0.00046\n",
            "Epoch: 26/40 Iteration: 6360 | Train loss: 0.00007\n",
            "Epoch: 26/40 Iteration: 6380 | Train loss: 0.00055\n",
            "Epoch: 26/40 Iteration: 6400 | Train loss: 0.00043\n",
            "Epoch: 26/40 Iteration: 6420 | Train loss: 0.00045\n",
            "Epoch: 26/40 Iteration: 6440 | Train loss: 0.00353\n",
            "Epoch: 26/40 Iteration: 6460 | Train loss: 0.00004\n",
            "Epoch: 26/40 Iteration: 6480 | Train loss: 0.00010\n",
            "Epoch: 26/40 Iteration: 6500 | Train loss: 0.00014\n",
            "Epoch: 27/40 Iteration: 6520 | Train loss: 0.00009\n",
            "Epoch: 27/40 Iteration: 6540 | Train loss: 0.00011\n",
            "Epoch: 27/40 Iteration: 6560 | Train loss: 0.00087\n",
            "Epoch: 27/40 Iteration: 6580 | Train loss: 0.00006\n",
            "Epoch: 27/40 Iteration: 6600 | Train loss: 0.00006\n",
            "Epoch: 27/40 Iteration: 6620 | Train loss: 0.00013\n",
            "Epoch: 27/40 Iteration: 6640 | Train loss: 0.00003\n",
            "Epoch: 27/40 Iteration: 6660 | Train loss: 0.00018\n",
            "Epoch: 27/40 Iteration: 6680 | Train loss: 0.00005\n",
            "Epoch: 27/40 Iteration: 6700 | Train loss: 0.00012\n",
            "Epoch: 27/40 Iteration: 6720 | Train loss: 0.00005\n",
            "Epoch: 27/40 Iteration: 6740 | Train loss: 0.00013\n",
            "Epoch: 28/40 Iteration: 6760 | Train loss: 0.00017\n",
            "Epoch: 28/40 Iteration: 6780 | Train loss: 0.00009\n",
            "Epoch: 28/40 Iteration: 6800 | Train loss: 0.00005\n",
            "Epoch: 28/40 Iteration: 6820 | Train loss: 0.00010\n",
            "Epoch: 28/40 Iteration: 6840 | Train loss: 0.00013\n",
            "Epoch: 28/40 Iteration: 6860 | Train loss: 0.00004\n",
            "Epoch: 28/40 Iteration: 6880 | Train loss: 0.00021\n",
            "Epoch: 28/40 Iteration: 6900 | Train loss: 0.00006\n",
            "Epoch: 28/40 Iteration: 6920 | Train loss: 0.00087\n",
            "Epoch: 28/40 Iteration: 6940 | Train loss: 0.00015\n",
            "Epoch: 28/40 Iteration: 6960 | Train loss: 0.00003\n",
            "Epoch: 28/40 Iteration: 6980 | Train loss: 0.00003\n",
            "Epoch: 28/40 Iteration: 7000 | Train loss: 0.00006\n",
            "Epoch: 29/40 Iteration: 7020 | Train loss: 0.00006\n",
            "Epoch: 29/40 Iteration: 7040 | Train loss: 0.00005\n",
            "Epoch: 29/40 Iteration: 7060 | Train loss: 0.00004\n",
            "Epoch: 29/40 Iteration: 7080 | Train loss: 0.00023\n",
            "Epoch: 29/40 Iteration: 7100 | Train loss: 0.00012\n",
            "Epoch: 29/40 Iteration: 7120 | Train loss: 0.00006\n",
            "Epoch: 29/40 Iteration: 7140 | Train loss: 0.00001\n",
            "Epoch: 29/40 Iteration: 7160 | Train loss: 0.00006\n",
            "Epoch: 29/40 Iteration: 7180 | Train loss: 0.00002\n",
            "Epoch: 29/40 Iteration: 7200 | Train loss: 0.00009\n",
            "Epoch: 29/40 Iteration: 7220 | Train loss: 0.00006\n",
            "Epoch: 29/40 Iteration: 7240 | Train loss: 0.00004\n",
            "Epoch: 30/40 Iteration: 7260 | Train loss: 0.00004\n",
            "Epoch: 30/40 Iteration: 7280 | Train loss: 0.00002\n",
            "Epoch: 30/40 Iteration: 7300 | Train loss: 0.00002\n",
            "Epoch: 30/40 Iteration: 7320 | Train loss: 0.00007\n",
            "Epoch: 30/40 Iteration: 7340 | Train loss: 0.00009\n",
            "Epoch: 30/40 Iteration: 7360 | Train loss: 0.00003\n",
            "Epoch: 30/40 Iteration: 7380 | Train loss: 0.00010\n",
            "Epoch: 30/40 Iteration: 7400 | Train loss: 0.00004\n",
            "Epoch: 30/40 Iteration: 7420 | Train loss: 0.00013\n",
            "Epoch: 30/40 Iteration: 7440 | Train loss: 0.00004\n",
            "Epoch: 30/40 Iteration: 7460 | Train loss: 0.00001\n",
            "Epoch: 30/40 Iteration: 7480 | Train loss: 0.00002\n",
            "Epoch: 30/40 Iteration: 7500 | Train loss: 0.00004\n",
            "Epoch: 31/40 Iteration: 7520 | Train loss: 0.00024\n",
            "Epoch: 31/40 Iteration: 7540 | Train loss: 0.00002\n",
            "Epoch: 31/40 Iteration: 7560 | Train loss: 0.00021\n",
            "Epoch: 31/40 Iteration: 7580 | Train loss: 0.00002\n",
            "Epoch: 31/40 Iteration: 7600 | Train loss: 0.00008\n",
            "Epoch: 31/40 Iteration: 7620 | Train loss: 0.00012\n",
            "Epoch: 31/40 Iteration: 7640 | Train loss: 0.00000\n",
            "Epoch: 31/40 Iteration: 7660 | Train loss: 0.00006\n",
            "Epoch: 31/40 Iteration: 7680 | Train loss: 0.00001\n",
            "Epoch: 31/40 Iteration: 7700 | Train loss: 0.00002\n",
            "Epoch: 31/40 Iteration: 7720 | Train loss: 0.00004\n",
            "Epoch: 31/40 Iteration: 7740 | Train loss: 0.00003\n",
            "Epoch: 32/40 Iteration: 7760 | Train loss: 0.00003\n",
            "Epoch: 32/40 Iteration: 7780 | Train loss: 0.00002\n",
            "Epoch: 32/40 Iteration: 7800 | Train loss: 0.00001\n",
            "Epoch: 32/40 Iteration: 7820 | Train loss: 0.00063\n",
            "Epoch: 32/40 Iteration: 7840 | Train loss: 0.00018\n",
            "Epoch: 32/40 Iteration: 7860 | Train loss: 0.00002\n",
            "Epoch: 32/40 Iteration: 7880 | Train loss: 0.00020\n",
            "Epoch: 32/40 Iteration: 7900 | Train loss: 0.00005\n",
            "Epoch: 32/40 Iteration: 7920 | Train loss: 0.00007\n",
            "Epoch: 32/40 Iteration: 7940 | Train loss: 0.00002\n",
            "Epoch: 32/40 Iteration: 7960 | Train loss: 0.00001\n",
            "Epoch: 32/40 Iteration: 7980 | Train loss: 0.00001\n",
            "Epoch: 32/40 Iteration: 8000 | Train loss: 0.00003\n",
            "Epoch: 33/40 Iteration: 8020 | Train loss: 0.00010\n",
            "Epoch: 33/40 Iteration: 8040 | Train loss: 0.00005\n",
            "Epoch: 33/40 Iteration: 8060 | Train loss: 0.00003\n",
            "Epoch: 33/40 Iteration: 8080 | Train loss: 0.00004\n",
            "Epoch: 33/40 Iteration: 8100 | Train loss: 0.00002\n",
            "Epoch: 33/40 Iteration: 8120 | Train loss: 0.00002\n",
            "Epoch: 33/40 Iteration: 8140 | Train loss: 0.00000\n",
            "Epoch: 33/40 Iteration: 8160 | Train loss: 0.00001\n",
            "Epoch: 33/40 Iteration: 8180 | Train loss: 0.00002\n",
            "Epoch: 33/40 Iteration: 8200 | Train loss: 0.00002\n",
            "Epoch: 33/40 Iteration: 8220 | Train loss: 0.00002\n",
            "Epoch: 33/40 Iteration: 8240 | Train loss: 0.00080\n",
            "Epoch: 34/40 Iteration: 8260 | Train loss: 0.00004\n",
            "Epoch: 34/40 Iteration: 8280 | Train loss: 0.00001\n",
            "Epoch: 34/40 Iteration: 8300 | Train loss: 0.00001\n",
            "Epoch: 34/40 Iteration: 8320 | Train loss: 0.00003\n",
            "Epoch: 34/40 Iteration: 8340 | Train loss: 0.00003\n",
            "Epoch: 34/40 Iteration: 8360 | Train loss: 0.00002\n",
            "Epoch: 34/40 Iteration: 8380 | Train loss: 0.00008\n",
            "Epoch: 34/40 Iteration: 8400 | Train loss: 0.00012\n",
            "Epoch: 34/40 Iteration: 8420 | Train loss: 0.00017\n",
            "Epoch: 34/40 Iteration: 8440 | Train loss: 0.00005\n",
            "Epoch: 34/40 Iteration: 8460 | Train loss: 0.00000\n",
            "Epoch: 34/40 Iteration: 8480 | Train loss: 0.00001\n",
            "Epoch: 34/40 Iteration: 8500 | Train loss: 0.00007\n",
            "Epoch: 35/40 Iteration: 8520 | Train loss: 0.00002\n",
            "Epoch: 35/40 Iteration: 8540 | Train loss: 0.00002\n",
            "Epoch: 35/40 Iteration: 8560 | Train loss: 0.00005\n",
            "Epoch: 35/40 Iteration: 8580 | Train loss: 0.00005\n",
            "Epoch: 35/40 Iteration: 8600 | Train loss: 0.00000\n",
            "Epoch: 35/40 Iteration: 8620 | Train loss: 0.00000\n",
            "Epoch: 35/40 Iteration: 8640 | Train loss: 0.00001\n",
            "Epoch: 35/40 Iteration: 8660 | Train loss: 0.00001\n",
            "Epoch: 35/40 Iteration: 8680 | Train loss: 0.00001\n",
            "Epoch: 35/40 Iteration: 8700 | Train loss: 0.00001\n",
            "Epoch: 35/40 Iteration: 8720 | Train loss: 0.00001\n",
            "Epoch: 35/40 Iteration: 8740 | Train loss: 0.00000\n",
            "Epoch: 36/40 Iteration: 8760 | Train loss: 0.00001\n",
            "Epoch: 36/40 Iteration: 8780 | Train loss: 0.00000\n",
            "Epoch: 36/40 Iteration: 8800 | Train loss: 0.00000\n",
            "Epoch: 36/40 Iteration: 8820 | Train loss: 0.00001\n",
            "Epoch: 36/40 Iteration: 8840 | Train loss: 0.00000\n",
            "Epoch: 36/40 Iteration: 8860 | Train loss: 0.00001\n",
            "Epoch: 36/40 Iteration: 8880 | Train loss: 0.00001\n",
            "Epoch: 36/40 Iteration: 8900 | Train loss: 0.00001\n",
            "Epoch: 36/40 Iteration: 8920 | Train loss: 0.00005\n",
            "Epoch: 36/40 Iteration: 8940 | Train loss: 0.00002\n",
            "Epoch: 36/40 Iteration: 8960 | Train loss: 0.00000\n",
            "Epoch: 36/40 Iteration: 8980 | Train loss: 0.00001\n",
            "Epoch: 36/40 Iteration: 9000 | Train loss: 0.00011\n",
            "Epoch: 37/40 Iteration: 9020 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9040 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9060 | Train loss: 0.00003\n",
            "Epoch: 37/40 Iteration: 9080 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9100 | Train loss: 0.00014\n",
            "Epoch: 37/40 Iteration: 9120 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9140 | Train loss: 0.00000\n",
            "Epoch: 37/40 Iteration: 9160 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9180 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9200 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9220 | Train loss: 0.00001\n",
            "Epoch: 37/40 Iteration: 9240 | Train loss: 0.00001\n",
            "Epoch: 38/40 Iteration: 9260 | Train loss: 0.00002\n",
            "Epoch: 38/40 Iteration: 9280 | Train loss: 0.00000\n",
            "Epoch: 38/40 Iteration: 9300 | Train loss: 0.00000\n",
            "Epoch: 38/40 Iteration: 9320 | Train loss: 0.00000\n",
            "Epoch: 38/40 Iteration: 9340 | Train loss: 0.00001\n",
            "Epoch: 38/40 Iteration: 9360 | Train loss: 0.00001\n",
            "Epoch: 38/40 Iteration: 9380 | Train loss: 0.00001\n",
            "Epoch: 38/40 Iteration: 9400 | Train loss: 0.00001\n",
            "Epoch: 38/40 Iteration: 9420 | Train loss: 0.00008\n",
            "Epoch: 38/40 Iteration: 9440 | Train loss: 0.00001\n",
            "Epoch: 38/40 Iteration: 9460 | Train loss: 0.00000\n",
            "Epoch: 38/40 Iteration: 9480 | Train loss: 0.00007\n",
            "Epoch: 38/40 Iteration: 9500 | Train loss: 0.00000\n",
            "Epoch: 39/40 Iteration: 9520 | Train loss: 0.00001\n",
            "Epoch: 39/40 Iteration: 9540 | Train loss: 0.00001\n",
            "Epoch: 39/40 Iteration: 9560 | Train loss: 0.00001\n",
            "Epoch: 39/40 Iteration: 9580 | Train loss: 0.00016\n",
            "Epoch: 39/40 Iteration: 9600 | Train loss: 0.00000\n",
            "Epoch: 39/40 Iteration: 9620 | Train loss: 0.00000\n",
            "Epoch: 39/40 Iteration: 9640 | Train loss: 0.00000\n",
            "Epoch: 39/40 Iteration: 9660 | Train loss: 0.00001\n",
            "Epoch: 39/40 Iteration: 9680 | Train loss: 0.00000\n",
            "Epoch: 39/40 Iteration: 9700 | Train loss: 0.00000\n",
            "Epoch: 39/40 Iteration: 9720 | Train loss: 0.00000\n",
            "Epoch: 39/40 Iteration: 9740 | Train loss: 0.00000\n",
            "Epoch: 40/40 Iteration: 9760 | Train loss: 0.00001\n",
            "Epoch: 40/40 Iteration: 9780 | Train loss: 0.00000\n",
            "Epoch: 40/40 Iteration: 9800 | Train loss: 0.00000\n",
            "Epoch: 40/40 Iteration: 9820 | Train loss: 0.00000\n",
            "Epoch: 40/40 Iteration: 9840 | Train loss: 0.00001\n",
            "Epoch: 40/40 Iteration: 9860 | Train loss: 0.00000\n",
            "Epoch: 40/40 Iteration: 9880 | Train loss: 0.00001\n",
            "Epoch: 40/40 Iteration: 9900 | Train loss: 0.00002\n",
            "Epoch: 40/40 Iteration: 9920 | Train loss: 0.00000\n",
            "Epoch: 40/40 Iteration: 9940 | Train loss: 0.00001\n",
            "Epoch: 40/40 Iteration: 9960 | Train loss: 0.00000\n",
            "Epoch: 40/40 Iteration: 9980 | Train loss: 0.00000\n",
            "Epoch: 40/40 Iteration: 10000 | Train loss: 0.00000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " ## Test: \n",
        "preds = rnn.predict(X_test)\n",
        "y_true = y_test[:len(preds)]\n",
        "print('Test Acc.: %.3f' % (\n",
        "      np.sum(preds == y_true) / len(y_true)))\n",
        "## Get probabilities:\n",
        "proba = rnn.predict(X_test, return_proba=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "THn-L322QvTR",
        "outputId": "08fb57a4-0dd6-4250-8eb9-3d95a6b5ca77"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./model/sentiment-39.ckpt\n",
            "Test Acc.: 0.853\n",
            "INFO:tensorflow:Restoring parameters from ./model/sentiment-39.ckpt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(proba)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpGOnYgNQvWa",
        "outputId": "1189dbbf-eb14-451d-ee13-3cf2608d4c9d"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6.6900965e-09 1.0000000e+00 9.9999332e-01 ... 3.7021758e-07 1.0000000e+00\n",
            " 1.6003986e-08]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(rnn.predict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYZn2ESnSXyZ",
        "outputId": "e67c0f1d-32d1-497d-9e51-bc8dfb2c4019"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method SentimentRNN.predict of <__main__.SentimentRNN object at 0x7fdee00cd190>>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLO5B_haSX3Y",
        "outputId": "287c467c-2d52-4daf-e821-7b69f3cd5515"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1451,     3,     1, ...,     9,   175,     2],\n",
              "       [    0,     0,     0, ..., 14606,   109,     2],\n",
              "       [    0,     0,     0, ...,   117,   117,   117],\n",
              "       ...,\n",
              "       [    0,     0,     0, ...,    17,    24,     2],\n",
              "       [    6,  3003,    74, ...,    66,  3719,     2],\n",
              "       [   80,    90,   273, ...,  1458,   852,     2]])"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    }
  ]
}